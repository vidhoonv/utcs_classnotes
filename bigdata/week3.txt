2 Feb 2015
==========

Unit Tests
==========
- There is a test dir in the directory structure to place tests.
- MapDriver, ReduceDriver are used to test them as isolated units from MRUnit
- as part of setup create map/reduce classes and also drivers
	- for example mapDriver = MapDriver.newMapDriver(mapper)
	- mapDriver.withInput(...)
	- mapDriver.withOutput(...)
	- mapDriver.runTest()
	- same flow for reduceDriver
- run hadopp with Amazon AWS for testing purposes (like simulation)
- mvn build will compile code and run these tests locally (saves time)

Summarization Design Pattern
============================
- Counting is common MR task
- Also statistical measures like mean, median, mode etc.
- mappers do local counts, reducers sum up
- combiners - very helpful 
	- to sum up counts locally 
	- reduce data sent to reducer(13000 "hello 1" entries vs 1 "hello 13000" entry)


Assignment 2
===========
- For Mean, we need
	- Total number of occurences of the word
	- Number of paragraphs containing the word

- Input has '\n' at end of paragraph (formatted)
-For variance,
	-formula is E[(X-mean)^2] or summation i=1 to n [(xi-mean)^2]/n
	- Using algebra, can be simplified to E[X^2]-(E[X])^2
	- X is the  number of times the word appeared in the paragraph
	- So E[X^2] is just like mean
	- We also have the mean - just square it
	- We have everything to find variance now
	- Just one pass is required 

Mapper
======
- For every word output,
	- number of paragraphs
	- total count
	- sum of squares of count

- context.write(word, (three LongWritables))
- how to write three LongWritables 
	- can be passed as text
	- create an object and put three numbers
- write() method and readFields()
- LongArrayWritable is needed
	- hadoop provides ArrayWritable
	- provides get(),getValueClass(), setWritable(), toArray(). toString()
	- implement this class as an extension of ArrayWritable
	- indicate the type of the array in constructor
		- like this: super(LongWritable.class)
	-implement getValueArray() to get array of longs from LongArrayWritable

Reducer
=======
- For every word output
	- mean
	- variance
- output of mapper looks like
	- the {1,7,49}, the {1,2,4}
- reducer needs to sum all three values for each word
- then it calculates mean and variance which are floating point types
- DoubleArrayWritable can extend ArrrayWritable 
	- ** important to override toString() method because while hadoop writes output it converts toString() for writing.

Combiner
========
- Use them to sum same word in different paragraphs locally before sending to reducer
- In this case, combiner and reducer are semantically different
	- combiner  only sums mapper output
	- reducer needs to also calculate mean and variance
- Combiner takes input like reducer and output like mapper

Dataset
=======
- there is a lot of quotations
- strip out all punctuations, convert to lowercase
- semicolon, double dashses, ... etc.
- do this para by para in mapper before tokenizing


4 Feb 2015
==========

Assignment 2 stuff
==================
- Create assign2 inside the same package as assign1
- Update the pom xml file as per needs for assign1
- tests for each component should be under the same package tree structure
- ex: 
	main
		com.refactorlabs.cs378.assign1
		com.refactorlabs.cs378.assign2
	test
		com.refactorlabs.cs378.assign1
		com.refactorlabs.cs378.assign2

- maven shade (pom.xml)
==============

- Used to include JARs of all dependencies (recursively)
	- some JARS can be excluded using <scope>provided</scope> withing the <dependency>

Summarization pattern (Contd.,)
=====================
- purpose of keys-> different between inputs to combiner and reducer for grouping
- For finding min and max length of  paras 
	- key can be anything - no need to group or differentiate inputs
	- combiner will get list of value pairs from mapper for each paragraph 
	- combiner will find min,max from the list of values from mapper
	- reducer does the same using output of combiner as input
	- in this case, both combiner and reducer have exact same functionality but in the case of assignment 2
		- combiner will just provide sum of lengths and sum of squares of lengths
		- reducer will compute mean , median using the sums
- Median
	- get values, sort them, pick middle
	- sort is challenging - needs all numbers in one machine or awarness to actually pick the middle
	- so we store, <length,1> in mapper. Something like wordcount but here we have para len, count instead of word count
	- if we need only one reducer, then use same key for all entries in mapper, combiner
	- SortedMapWritable class - sort the <key,value> entries by keys
	- Example in chapter 2

Compute Min,Max and Median all in one pass
==========================================
- (key,value) : ("pstats",<length,1>)
-  pick first and last entries in reducer

Using counters
- can use counters to instrument/debug
- accessed by groups
- limited number of counters available

Jobtracker and Tasktracker (hadoop internals)
==========================
- Job object contains info abt hadoop job - config written to HDFS
- Job tracker spins required number of mappers, reducers
- Task tracker runs on mappers and reducers
- Job tracker talks to task trackers in mapper and reducer to schedule the mapper and reducer tasks for a particular job
- Task tracker tracks map/reduce task and job tracker interacts with task tracker - used to handle fault tolerance
- when job tracker sees map tasks completed, it starts reduce tasks
- map tasks are assigned to task trackers that are close to input split location
	- data local preferred
	- same local rack preferred next
- reduce tasks can go anywhere - why? less data
- task tracker and job tracker have heartbeat msgs - for fault tolerance

Measuring progress of task
==========================
- even before mappers finish execution, sort and shuffle phase of reducers can start. So it is common to see reducers start making progress before all mappers are finished

